# -*- coding: utf-8 -*-
"""PerceptronImp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WvcO8hHs34AG8IwO8azxUPAhajo0d328

Import all relevant libraries and functions

ECE421 Lab1 Part 1
By: Shawn Zhai, Victor Wu
Date: Jan 30th, 2023
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Perceptron
from sklearn.metrics import confusion_matrix

#the function runs the Pocket Algorithm on the inputs with a maximum epoch of 5000
def fit_perceptron(X_train, y_train):
    #Add implementation here

    #modify input X_train matrix to account for offset term
    column_ones = np.ones((X_train.shape[0],1))
    X_train_modified = np.hstack((column_ones, X_train))

    #weight_temp is initialized as zero vector 
    #and iteratively updated when running the pocket algorithm
    #weight_best tracks which iteration of weight_temp had the minimum loss
    weight_temp = np.zeros(X_train_modified.shape[1])
    weight_best = np.zeros(X_train_modified.shape[1])
    
    #initially, loss is 100%. As pocket algorithm runs, 
    #it iteratively gets smaller.
    loss_best = 1

    #Pocket algorithm
    #5000 epoch
    for it in range(5000):

        #go over the dataset
        for i in range(X_train_modified.shape[0]):

            #compute y-hat
            wTx = np.dot(weight_temp, X_train_modified[i])

            #misclassifed if perdiction (wTx) is different from ith element's 
            #ground-truth observation 
            if (wTx > 0 and y_train[i] == -1) or (wTx <= 0 and y_train[i] == 1):    
                weight_temp += y_train[i] * X_train_modified[i]

                #if we get a better loss, make it the best
                loss_temp = errorPer(X_train_modified, y_train, weight_temp)
                if loss_temp < loss_best:
                    weight_best = weight_temp
                    loss_best = loss_temp

    return weight_best

#the function calculates the percentage error of a weight vector w  
#on the training samples X_train using the counting loss function
def errorPer(X_train,y_train,w):

    #Initialized loss to 0
    loss = 0.0

    #go through all samples, check for misclassification
    for i in range(X_train.shape[0]):
        wTx = np.dot(w, X_train[i])

        #misclassfied, count up error
        if wTx == 0 or (wTx > 0 and y_train[i] == -1) or (wTx < 0 and y_train[i] == 1):
            loss += 1

    #return the average error
    return loss / X_train.shape[0]

#create a  2Ã—2  confusion matrix, with the following form:
#[True Negative  , False Positive]
#[False Negative  , True Positive]
def confMatrix(X_train,y_train,w):

    column_ones = np.ones((X_train.shape[0],1))
    X_train_modified = np.hstack((column_ones, X_train))

    #create empty 2x2 matrix
    cM = np.zeros((2,2), dtype=np.int8)

    #between every perdicted y and ground-truth y, check which case
    #of confusion matrix the result should be in
    for i in range(X_train_modified.shape[0]):
        yi_hat = pred(X_train_modified[i],w)

        #true negative
        if yi_hat == -1 and y_train[i] == -1:
            cM[0][0] += 1
        
        #false positive
        elif yi_hat == 1 and y_train[i] == -1:
            cM[0][1] += 1

        #false negative
        elif yi_hat == -1 and y_train[i] == 1:
            cM[1][0] += 1
        
        #true positive
        elif yi_hat == 1 and y_train[i] == 1:
            cM[1][1] += 1

    return cM

#the function makes a perdiction,  y-hat  for a sample  xi  in X_train using a weight vector w
def pred(X_train,w):

    #compute wTx
    wTx = np.dot(X_train, w)

    #sign function
    if wTx > 0:
        return 1
    
    else:
        return -1

#The function uses the scikit-learn Python library to create and train 
#a binary linear classifier model based on input samples and input observations.
#The model's effectiveness is measured by comparing its perdictions of 
#X_test samples  y-predict , and ground-truth observations Y_train.
#The comparison result is stored in a confusion matrix.
def test_SciKit(X_train, X_test, Y_train, Y_test):

    #pocket algorithm: iterate 5000 times 
    clf = Perceptron(max_iter = 5000, tol = None)

    #fit the model
    clf.fit(X_train, Y_train)

    #perdiction
    y_pred = clf.predict(X_test)

    #construct confMatrix between Y_test and y_pred
    cM = confusion_matrix(Y_test, y_pred)
    return cM

def test_Part1():
    from sklearn.datasets import load_iris
    X_train, y_train = load_iris(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(X_train[50:],y_train[50:],test_size=0.2)

    #Set the labels to +1 and -1
    y_train[y_train == 1] = 1
    y_train[y_train != 1] = -1
    y_test[y_test == 1] = 1
    y_test[y_test != 1] = -1

    #Pocket algorithm using Numpy
    w=fit_perceptron(X_train,y_train)
    cM=confMatrix(X_test,y_test,w)

    #Pocket algorithm using scikit-learn
    sciKit=test_SciKit(X_train, X_test, y_train, y_test)
    
    #Print the result
    print ('--------------Test Result-------------------')
    print("Confusion Matrix is from Part 1a is: ",cM)
    print("Confusion Matrix from Part 1b is:",sciKit)

test_Part1()