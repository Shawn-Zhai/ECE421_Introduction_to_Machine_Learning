# -*- coding: utf-8 -*-
"""NeuralNetPyTorch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Ahns9l6upWrESaQjXZYbw761Gy7a97N
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt
import numpy as np

# Function for loading notMNIST Dataset
def loadData(datafile = "notMNIST.npz"):
    with np.load(datafile) as data:
        Data, Target = data["images"].astype(np.float32), data["labels"]
        np.random.seed(521)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data = Data[randIndx] / 255.0
        Target = Target[randIndx]
        trainData, trainTarget = Data[:10000], Target[:10000]
        validData, validTarget = Data[10000:16000], Target[10000:16000]
        testData, testTarget = Data[16000:], Target[16000:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

# Custom Dataset class. 
class notMNIST(Dataset):
    def __init__(self, annotations, images, transform=None, target_transform=None):
        self.img_labels = annotations
        self.imgs = images
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        image = self.imgs[idx]
        label = self.img_labels[idx]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label

#Define FNN
class FNN(nn.Module):
    def __init__(self, drop_out_p=0.0):
        super(FNN, self).__init__()
        #TODO
        #DEFINE YOUR LAYERS HERE

        # Fully connected layer 784x10
        self.layer1 = nn.Linear(784, 10)

        # Fully connected layer 10x10
        self.layer2 = nn.Linear(10, 10)

        # Dropout layer
        self.dlayer = nn.Dropout(p = drop_out_p)

        # Fully connected layer 10x10
        self.layer3 = nn.Linear(10, 10)

    def forward(self, x):
        #TODO
        #DEFINE YOUR FORWARD FUNCTION HERE

        # Forward propagation following the described network architecture
        x0 = torch.flatten(x, start_dim=1)
        s1 = self.layer1(x0)
        x1 = F.relu(s1)
        s2 = self.layer2(x1)
        x2 = F.relu(s2)
        x2 = self.dlayer(x2)
        out = self.layer3(x2)

        return out

#Define CNN
class CNN(nn.Module):
    def __init__(self, drop_out_p=0.0):
        super(CNN, self).__init__()
        #TODO
        #DEFINE YOUR LAYERS HERE

        # Convolutional layer 1
        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 4)
        self.batchnorm1 = nn.BatchNorm2d(num_features = 32)
        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)
        
        # Convolutional layer 2
        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4)
        self.batchnorm2 = nn.BatchNorm2d(num_features = 64)
        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)

        # Dropout layer and 2 fully connect layers
        self.dlayer = nn.Dropout(p = drop_out_p)
        self.fulcon1 = nn.Linear(1024, 784)
        self.fulcon2 = nn.Linear(784, 10)

    def forward(self, x):
        #TODO
        #DEFINE YOUR FORWARD FUNCTION HERE

        # Forward propagation following the described network architecture

        # Propagating through convolutional layer 1
        s1 = self.conv1(x)
        x1 = F.relu(s1)
        x1 = self.batchnorm1(x1)
        x1 = self.maxpool1(x1)

        # Propagating through convolutional layer 2
        s2 = self.conv2(x1)
        x2 = F.relu(s2)
        x2 = self.batchnorm2(x2)
        x2 = self.maxpool2(x2)

        # Propagating through dropout layer and 2 fully connected layers
        x2 = torch.flatten(x2, start_dim=1)
        x2 = self.dlayer(x2)
        s3 = self.fulcon1(x2)
        x3 = F.relu(s3)
        out = self.fulcon2(x3)

        return out

# Compute accuracy
def get_accuracy(model, dataloader):
    
    model.eval()
    device = next(model.parameters()).device
    accuracy = 0.0
    total = 0.0
    
    with torch.no_grad():
        for data in dataloader:
            images, labels = data
            images = images.to(device)
            labels = labels.to(device) 
            # TODO
            # Return the accuracy

            logits = model(images)
            predicted = torch.max(logits, 1)[1]
            accuracy += (predicted == labels).sum().item()
            total += labels.size(0)

    return 100.0 * accuracy / total

# Commented out IPython magic to ensure Python compatibility.
def train(model, device, learning_rate, weight_decay, train_loader, val_loader, test_loader, num_epochs=50, verbose=False):
  #TODO
  # Define your cross entropy loss function here 
  # Use cross entropy loss
  criterion = nn.CrossEntropyLoss()
  
  #TODO
  # Define your optimizer here
  # Use AdamW optimizer, set the weights, learning rate and weight decay argument.
  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = weight_decay)

  acc_hist = {'train':[], 'val':[], 'test': []}

  for epoch in range(num_epochs):
    model = model.train()
    ## training step
    for i, (images, labels) in enumerate(train_loader):
        
        images = images.to(device)
        labels = labels.to(device)

        # TODO
        # Follow the step in the tutorial
        ## forward + backprop + loss
        logits = model(images)
        loss = criterion(logits, labels)
        optimizer.zero_grad()
        loss.backward()

        ## update model params
        optimizer.step()

    model.eval()
    acc_hist['train'].append(get_accuracy(model, train_loader))
    acc_hist['val'].append(get_accuracy(model, val_loader))
    acc_hist['test'].append(get_accuracy(model, test_loader))
    
    if verbose:
      print('Epoch: %d | Train Accuracy: %.2f | Validation Accuracy: %.2f | Test Accuracy: %.2f' \
#             %(epoch, acc_hist['train'][-1], acc_hist['val'][-1], acc_hist['test'][-1]))

  return model, acc_hist

def experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0.5, weight_decay=0.01, num_epochs=50, verbose=False):
  # Use GPU if it is available.
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

  # Inpute Batch size:
  BATCH_SIZE = 32

  # Convert images to tensor
  transform = transforms.Compose(
      [transforms.ToTensor()])

  # Get train, validation and test data loader.
  trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()

  train_data = notMNIST(trainTarget, trainData, transform=transform)
  val_data = notMNIST(validTarget, validData, transform=transform)
  test_data = notMNIST(testTarget, testData, transform=transform)


  train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
  val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)
  test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)

  # Specify which model to use
  if model_type == 'CNN':
    model = CNN(dropout_rate)
  elif model_type == 'FNN':
    model = FNN(dropout_rate)

  
  # Loading model into device
  model = model.to(device)
  criterion = nn.CrossEntropyLoss()
  model, acc_hist = train(model, device, learning_rate, weight_decay, train_loader, val_loader, test_loader, num_epochs=num_epochs, verbose=verbose)
  
  # Release the model from the GPU (else the memory wont hold up)
  model.cpu()

  return model, acc_hist

# Experiment 1
def compare_arch():

    # Get the evolution of training, validation, and testing accuracy
    CNNhistory = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0, weight_decay=0, num_epochs=50, verbose=False)[1]
    FNNhistory = experiment(model_type='FNN', learning_rate=0.0001, dropout_rate=0, weight_decay=0, num_epochs=50, verbose=False)[1]

    X = np.arange(1, 51)

    # CNN & FNN training accuracy plot
    Ycnn = np.array(CNNhistory['train'])
    Yfnn = np.array(FNNhistory['train'])
    plt.title('CNN vs. FNN Training Accuracy Evolution')
    plt.xlabel('Epoch')
    plt.ylabel('Training Accuracy')
    plt.plot(X, Ycnn, label = "CNN")
    plt.plot(X, Yfnn, label = "FNN")
    plt.legend()
    plt.savefig('EXP1_Training.png')
    plt.show()

    # CNN & FNN testing accuracy plot
    Ycnn = np.array(CNNhistory['test'])
    Yfnn = np.array(FNNhistory['test'])
    plt.title('CNN vs. FNN Testing Accuracy Evolution')
    plt.xlabel('Epoch')
    plt.ylabel('Testing Accuracy')
    plt.plot(X, Ycnn, label = "CNN")
    plt.plot(X, Yfnn, label = "FNN")
    plt.legend()
    plt.savefig('EXP1_Testing.png')
    plt.show()

# Experiment 2
def compare_dropout():
    CNNhistory05 = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0.5, weight_decay=0, num_epochs=50, verbose=False)[1]
    CNNhistory08 = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0.8, weight_decay=0, num_epochs=50, verbose=False)[1]
    CNNhistory095 = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0.95, weight_decay=0, num_epochs=50, verbose=False)[1]

    X = np.arange(1, 51)

    # CNN training accuracy plot using 3 models
    Y05 = np.array(CNNhistory05['train'])
    Y08 = np.array(CNNhistory08['train'])
    Y095 = np.array(CNNhistory095['train'])
    plt.title('CNN training accuracy plot using 3 models with different dropout rates')
    plt.xlabel('Epoch')
    plt.ylabel('Training Accuracy')
    plt.plot(X, Y05, label = "d = 0.5")
    plt.plot(X, Y08, label = "d = 0.8")
    plt.plot(X, Y095, label = "d = 0.95")
    plt.legend()
    plt.savefig('EXP2_Training.png')
    plt.show()

    # CNN testing accuracy plot using 3 models
    Y05 = np.array(CNNhistory05['test'])
    Y08 = np.array(CNNhistory08['test'])
    Y095 = np.array(CNNhistory095['test'])
    plt.title('CNN testing accuracy plot using 3 models with different dropout rates')
    plt.xlabel('Epoch')
    plt.ylabel('Testing Accuracy')
    plt.plot(X, Y05, label = "d = 0.5")
    plt.plot(X, Y08, label = "d = 0.8")
    plt.plot(X, Y095, label = "d = 0.95")
    plt.legend()
    plt.savefig('EXP2_Testing.png')
    plt.show()

def compare_l2():
    CNNhistory01 = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0, weight_decay=0.1, num_epochs=50, verbose=False)[1]
    CNNhistory10 = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0, weight_decay=1.0, num_epochs=50, verbose=False)[1]
    CNNhistory100 = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0, weight_decay=10.0, num_epochs=50, verbose=False)[1]

    X = np.arange(1, 51)

    # CNN training accuracy plot using 3 models
    Y01 = np.array(CNNhistory01['train'])
    Y10 = np.array(CNNhistory10['train'])
    Y100 = np.array(CNNhistory100['train'])
    plt.title('CNN training accuracy plot using 3 models \n with different weight decay values')
    plt.xlabel('Epoch')
    plt.ylabel('Training Accuracy')
    plt.plot(X, Y01, label = "wd = 0.1")
    plt.plot(X, Y10, label = "wd = 1.0")
    plt.plot(X, Y100, label = "wd = 10.0")
    plt.legend()
    plt.savefig('EXP3_Training.png')
    plt.show()

    # CNN testing accuracy plot using 3 models
    Y01 = np.array(CNNhistory01['test'])
    Y10 = np.array(CNNhistory10['test'])
    Y100 = np.array(CNNhistory100['test'])
    plt.title('CNN testing accuracy plot using 3 models \n with different weight decay values')
    plt.xlabel('Epoch')
    plt.ylabel('Testing Accuracy')
    plt.plot(X, Y01, label = "wd = 0.1")
    plt.plot(X, Y10, label = "wd = 1.0")
    plt.plot(X, Y100, label = "wd = 10.0")
    plt.legend()
    plt.savefig('EXP3_Testing.png')
    plt.show()

# Perform 3 experiments
compare_arch()
compare_dropout()
compare_l2()