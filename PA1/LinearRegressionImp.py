# -*- coding: utf-8 -*-
"""ECE421 Lab1 part2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ak9X1p47eJMFD28egds4ZnTnEFeORJ38

ECE421 Lab1 Part 2
By: Shawn Zhai, Victor Wu
Date: Jan 30th, 2023
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn import linear_model
from sklearn.metrics import mean_squared_error

#This function computes the parameters w of a linear plane 
#which best fits the training dataset.
def fit_LinRegr(X_train, y_train):

    #modify input X_train matrix to account for offset term
    column_ones = np.ones((X_train.shape[0],1))
    X = np.hstack((column_ones, X_train))

    #X_t
    X_transpose = np.transpose(X)

    #(X_t * X)
    step1 = np.matmul(X_transpose, X)
    
    #NOTE: cannot use linalg.inv() because often times the data sample matrix
    #is not full column rank, therefore cannot be inverted
    #linalg.pinv() solves this problem because it returns an approximation for 
    #inverse of singular matrix
    
    #(X_t * X)-1
    #step2 = np.linalg.inv(step1)
    step2 = np.linalg.pinv(step1)

    #(X_t * X)-1 * X_t
    step3 = np.matmul(step2,X_transpose)
    
    #(X_t * X)-1 * X_t * y_train
    w_ideal = np.matmul(step3,y_train)

    return w_ideal

#this function computes the mean square error introduced by 
#the linear plane defined by w.
def mse(X_train,y_train,w):

    #modify input X_train matrix to account for offset term
    column_ones = np.ones((X_train.shape[0],1))
    X = np.hstack((column_ones, X_train))
    
    #Init mse to 0
    mse = 0.0

    #add up error for all data
    for i in range(X.shape[0]):
        mse += (pred(X[i],w) - y_train[i]) ** 2.0

    #return the average error
    return mse/X.shape[0]

#this function makes a prediction(yi-hat) for one data sample xi
#with a weight vector w
def pred(X_train,w):
    wTx = np.dot(X_train, w)
    return wTx

#this function use the scikit-learn library to train the linear regression model
def test_SciKit(X_train, X_test, Y_train, Y_test):

    #fit the model for the given training data
    lrm = linear_model.LinearRegression().fit(X_train, Y_train)

    #make a y-hat prediction for X_test
    y_pred = lrm.predict(X_test)

    #compute the mean square error between Y_test and y_predict
    mse = mean_squared_error(Y_test, y_pred)
    
    return mse

def subtestFn():
    # This function tests if your solution is robust against singular matrix

    # X_train has two perfectly correlated features
    X_train = np.asarray([[1, 2], [2, 4], [3, 6], [4, 8]])
    y_train = np.asarray([1,2,3,4])
    
    try:
      w=fit_LinRegr(X_train, y_train)
      print ("weights: ", w)
      print ("NO ERROR")
    except:
      print ("ERROR")

def testFn_Part2():
    X_train, y_train = load_diabetes(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size=0.2)

    print(X_train)

    w=fit_LinRegr(X_train, y_train)

    #Testing Part 2a
    e=mse(X_test,y_test,w)
    
    #Testing Part 2b
    scikit=test_SciKit(X_train, X_test, y_train, y_test)
    
    print("Mean squared error from Part 2a is ", e)
    print("Mean squared error from Part 2b is ", scikit)

print ('------------------subtestFn----------------------')
subtestFn()

print ('------------------testFn_Part2-------------------')
testFn_Part2()